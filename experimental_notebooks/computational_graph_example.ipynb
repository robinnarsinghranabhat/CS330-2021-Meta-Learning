{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "meta_learn",
   "display_name": "meta_learn",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 1. Simple Computational Graph Example with Pytorch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 1.1 Build the computaional graph as shown in figure"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SimpleGraph(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleGraph, self).__init__()\n",
    "        # === Initialize Weights === #\n",
    "        self.w1 = torch.nn.Parameter(data=torch.Tensor([5]), requires_grad=True)\n",
    "        self.w2 = torch.nn.Parameter(data=torch.Tensor([6]), requires_grad=True)\n",
    "    \n",
    "    def forward(self, a):\n",
    "        b = self.w1 * a\n",
    "        c = self.w2 * a\n",
    "        d = b + c\n",
    "        L = d.sum()\n",
    "        return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Gradient of Loss w.r.t w1=5.0 -> None\nThe Gradient of Loss w.r.t w2=6.0 -> None\n"
     ]
    }
   ],
   "source": [
    "net = SimpleGraph()\n",
    "a = torch.Tensor([5])\n",
    "out = net(a)\n",
    "\n",
    "### Print parameters and gradients\n",
    "### Gradients are None as we haven't calculated them yet\n",
    "\n",
    "for i in net.named_parameters():\n",
    "    param_name, param_tensor = i\n",
    "    print(f\"The Gradient of Loss w.r.t {param_name}={param_tensor.data.item()} -> {param_tensor.grad}\")"
   ]
  },
  {
   "source": [
    "### Compute the derivative and print params Again"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Gradient of Loss w.r.t w1=5.0 -> 5.0\nThe Gradient of Loss w.r.t w2=6.0 -> 5.0\n"
     ]
    }
   ],
   "source": [
    "out.backward(retain_graph=False)\n",
    "for i in net.named_parameters():\n",
    "    param_name, param_tensor = i\n",
    "    print(f\"The Gradient of Loss w.r.t {param_name}={param_tensor.data.item()} -> {param_tensor.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error : \nTrying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "## Try Doing backward again, This will give error ##\n",
    "try:\n",
    "    out.backward()\n",
    "except RuntimeError as e:\n",
    "    print(f'Error : \\n{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Gradient of Loss w.r.t w1=5.0 -> 5.0\nThe Gradient of Loss w.r.t w2=6.0 -> 5.0\n"
     ]
    }
   ],
   "source": [
    "# Repeating out.backward again will give error, as by default, \n",
    "# intermediate local gradients are cleared to save memory\n",
    "# To disable that, we give `retain_graph=True`\n",
    "net = SimpleGraph()\n",
    "a = torch.Tensor([5])\n",
    "out = net(a)\n",
    "\n",
    "out.backward(retain_graph=True)\n",
    "for i in net.named_parameters():\n",
    "    param_name, param_tensor = i\n",
    "    print(f\"The Gradient of Loss w.r.t {param_name}={param_tensor.data.item()} -> {param_tensor.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Gradient of Loss w.r.t w1=5.0 -> 10.0\nThe Gradient of Loss w.r.t w2=6.0 -> 10.0\n"
     ]
    }
   ],
   "source": [
    "## Let's repeat backward again and see what happens\n",
    "out.backward(retain_graph=True)\n",
    "for i in net.named_parameters():\n",
    "    param_name, param_tensor = i\n",
    "    print(f\"The Gradient of Loss w.r.t {param_name}={param_tensor.data.item()} -> {param_tensor.grad.item()}\")\n",
    "\n",
    "# As you can see, gradients got added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Gradient of Loss w.r.t w1=5.0 -> 15.0\nThe Gradient of Loss w.r.t w2=6.0 -> 15.0\n"
     ]
    }
   ],
   "source": [
    "## Doing this again in a new-cell, gradients further get added.\n",
    "out.backward(retain_graph=True)\n",
    "for i in net.named_parameters():\n",
    "    param_name, param_tensor = i\n",
    "    print(f\"The Gradient of Loss w.r.t {param_name}={param_tensor.data.item()} -> {param_tensor.grad.item()}\")"
   ]
  },
  {
   "source": [
    "### 1.2 Plot our Computational Graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'test_comp_graphs/simple_graph.png'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "from pathlib import Path\n",
    "\n",
    "fig_save = Path('./test_comp_graphs/')\n",
    "fig_save.mkdir(parents=True, exist_ok=True)\n",
    "make_dot(out, params=dict(list(net.named_parameters()))).render( fig_save / \"simple_graph\" , format=\"png\")"
   ]
  },
  {
   "source": [
    "## 2. Computaional Graph in a loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torch\n",
    "\n",
    "class SimpleRecurrentGraph(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SimpleRecurrentGraph, self).__init__()\n",
    "        # === Initialize Weights === #\n",
    "        self.w1 = torch.nn.Parameter(data=torch.Tensor([2]), requires_grad=True)\n",
    "        # === Initialize first hidden input as 1 === #\n",
    "        self.hidden = torch.tensor([1], requires_grad=False)\n",
    "        # self.hidden = torch.nn.Parameter(data=torch.Tensor([1]), requires_grad=False)\n",
    "    \n",
    "    def forward(self, a):\n",
    "        for inp in a:\n",
    "            hidden_next = self.w1 * self.hidden * inp\n",
    "            self.hidden = hidden_next\n",
    "\n",
    "        return hidden_next"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Output is : tensor([48.], grad_fn=<MulBackward0>)\nThe Gradient of Loss w.r.t w1=2.0 -> None\n"
     ]
    }
   ],
   "source": [
    "net = SimpleRecurrentGraph()\n",
    "a = [ torch.Tensor([1]), torch.Tensor([2]), torch.Tensor([3]) ]\n",
    "out = net(a)\n",
    "\n",
    "print(f'Output is : {out}')\n",
    "### Print parameters and gradients\n",
    "### Gradients are None as we haven't calculated them yet\n",
    "\n",
    "for i in net.named_parameters():\n",
    "    param_name, param_tensor = i\n",
    "    print(f\"The Gradient of Loss w.r.t {param_name}={param_tensor.data.item()} -> {param_tensor.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Gradient of Loss w.r.t w1=2.0 -> tensor([72.])\n"
     ]
    }
   ],
   "source": [
    "for i in net.named_parameters():\n",
    "    param_name, param_tensor = i\n",
    "    print(f\"The Gradient of Loss w.r.t {param_name}={param_tensor.data.item()} -> {param_tensor.grad}\")"
   ]
  },
  {
   "source": [
    "#### plot the graph"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'test_comp_graphs/simple_recurrent_graph.png'"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "from pathlib import Path\n",
    "\n",
    "fig_save = Path('./test_comp_graphs/')\n",
    "fig_save.mkdir(parents=True, exist_ok=True)\n",
    "make_dot(out, params=dict(list(net.named_parameters()))).render( fig_save / \"simple_recurrent_graph\" , format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}