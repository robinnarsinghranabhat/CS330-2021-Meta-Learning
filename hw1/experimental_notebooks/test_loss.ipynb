{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def softmax(vec):\n",
    "    return torch.exp(vec)/ torch.sum(torch.exp(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : In pytorch > we represent 0,1,2 .. as : [1,0,0] , [0,1,0] , [0,0,1] <br>\n",
    "[0,0,0] doesn't represent anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1 : Cross entroy in simple batch * one_hot_dimension || number_of_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3]) torch.Size([2]) tensor(1.1029)\n"
     ]
    }
   ],
   "source": [
    "## input for shape : Batch-Size * one-hot-out-dimension\n",
    "\n",
    "## this is : Batch_size * seq_length * one-hot-out-dimension\n",
    "# for calculating loss in many-to-many scenarios \n",
    "# say, LSTM outputting something for 2 time steps in one training example\n",
    "# and we have batch of 2\n",
    "inp = torch.tensor(\n",
    "         [\n",
    "          [1.0 , 0, 0], # this means 0\n",
    "          [0.0  ,1, 0], # this means 1\n",
    "         ]\n",
    "\n",
    ")\n",
    "\n",
    "inp.shape\n",
    "\n",
    "\n",
    "label = torch.tensor(\n",
    "        [0,1], ).long()\n",
    "\n",
    "loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "output = loss(inp, label)\n",
    "\n",
    "print(inp.shape, label.shape, output)\n",
    "# NOTE : try changning labels . loss will only increase. this is the only case with minimum loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  PART 1 manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1029480444875994\n"
     ]
    }
   ],
   "source": [
    "# first take softmax in input\n",
    "for i in range(2):\n",
    "        inp[i] = softmax( inp[i] )\n",
    "        \n",
    "\n",
    "## for first batch : Expected Output > 1,0,0\n",
    "\n",
    "loss_at_first_example = - ( np.log(0.5761) * 1 +  np.log(0.2119) * 0 + np.log(0.2119) * 0 )\n",
    "\n",
    "## for second batch : Expected Output > 0,1,0\n",
    "loss_at_second_example = - ( np.log(0.2119) * 0 +  np.log(0.5761) * 1 + np.log(0.2119) * 0 )\n",
    "\n",
    "print( loss_at_first_example + loss_at_second_example   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hence same loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 :  Cross Entropy in RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 0., 0.],\n",
       "          [0., 0., 0., 0.],\n",
       "          [0., 0., 0., 1.]],\n",
       " \n",
       "         [[0., 0., 0., 0.],\n",
       "          [1., 0., 0., 0.],\n",
       "          [0., 1., 0., 1.]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input of Batch_Size * Number_classes\n",
    "inp = torch.tensor(\n",
    "    [ \n",
    "        # First example : batch 1\n",
    "        # list of no_of_class dimensional output, at each timestep\n",
    "        [ [1,0,0],  [1,0,0],  [0,0,0],  [0,0,1]  ] , \n",
    "        \n",
    "        # Similarly a second Example : batch 2\n",
    "        [ [ 0, 1.0, 0  ],  [0,0,1] ,  [0,0,0] ,  [0,0,1]  ] \n",
    "     ]\n",
    "\n",
    ")\n",
    "\n",
    "# Convert to format pytorch expects : as per documentation\n",
    "inp = torch.permute(inp ,(0, 2, 1) )\n",
    "inp, inp.shape ## batch_size * num_of_classes * sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch_size * output_at_each_time_step ( where output between 0,1,2) in our case for now\n",
    "label = torch.tensor(\n",
    "        [[0,1,2,2],\n",
    "         [1,1,0,0],\n",
    "        ]\n",
    "        ).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 4]) tensor(8.5059)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='sum')\n",
    "output = loss(inp, label)\n",
    "\n",
    "print(inp.shape, label.shape, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Manual Check what's happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size : 2\n",
      "Num Classes / output_vector_space  : 3\n",
      "Time Steps : 4\n"
     ]
    }
   ],
   "source": [
    "b, n_c, t = inp.shape\n",
    "print(f'Batch size : {b}')\n",
    "print(f'Num Classes / output_vector_space  : {n_c}')\n",
    "print(f'Time Steps : {t}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 0\n",
      "\tInput : tensor([1., 0., 0.]) Expected : tensor([1., 0., 0.]) Loss : 0.5514447689056396\n",
      "\tInput : tensor([1., 0., 0.]) Expected : tensor([0., 1., 0.]) Loss : 1.5514447689056396\n",
      "\tInput : tensor([0., 0., 0.]) Expected : tensor([0., 0., 1.]) Loss : 1.0986123085021973\n",
      "\tInput : tensor([0., 0., 1.]) Expected : tensor([0., 0., 1.]) Loss : 0.5514447689056396\n",
      "batch : 1\n",
      "\tInput : tensor([0., 1., 0.]) Expected : tensor([0., 1., 0.]) Loss : 0.5514447689056396\n",
      "\tInput : tensor([0., 0., 1.]) Expected : tensor([0., 1., 0.]) Loss : 1.5514447689056396\n",
      "\tInput : tensor([0., 0., 0.]) Expected : tensor([1., 0., 0.]) Loss : 1.0986123085021973\n",
      "\tInput : tensor([0., 0., 1.]) Expected : tensor([1., 0., 0.]) Loss : 1.5514447689056396\n",
      "Total Loss across whole batch :  tensor(8.5059)\n"
     ]
    }
   ],
   "source": [
    "total_loss_across_batch = 0\n",
    "\n",
    "for b in range(2):\n",
    "    print(f'batch : {b}')\n",
    "    for t in range(4):\n",
    "        \n",
    "        label_exact = label[b][t].item()\n",
    "        label_full = torch.eye(3)[label_exact]\n",
    "        \n",
    "        \n",
    "        # calculate cross-entropy loss for one example\n",
    "        arbit =  softmax(inp[b, :, t])\n",
    "#         print(arbit)\n",
    "        summed = 0\n",
    "        for i in range(n_c):\n",
    "            loss_intermediate = - ( torch.log( arbit[i] ) * label_full[i]  )\n",
    "            summed += loss_intermediate\n",
    "        \n",
    "        total_loss_across_batch += summed\n",
    "        \n",
    "        print(f'\\tInput : {inp[b, :, t]} Expected : {label_full} Loss : {summed}')\n",
    "        \n",
    "\n",
    "print('Total Loss across whole batch : ', total_loss_across_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## good example for 3d input loss calculation in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4105)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "batch_size = 2\n",
    "max_len = 5\n",
    "num_classes = 4\n",
    "pred = torch.randn([batch_size, num_classes, max_len ])\n",
    "label = torch.randint(0, num_classes,[batch_size, max_len])\n",
    "pred = nn.Softmax(dim = 2)(pred)\n",
    "criterion(pred, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_learn",
   "language": "python",
   "name": "meta_learn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
